[{"content":"Deep Transformer Soft Actor-Critic Network for Reinforcement Learning Utilize Transformer as memory module for both Actor and Policy networks Hyperparameter tuning for SAC performance Sentiment Analysis on MyAnimeList User Ratings MyAnimeList is a popular anime rating website. Predict user rating based on review using Recurrent Neural Network (RNN) Setup a data-mining pipeline utilizing self-hosted REST API with a Redis server for caching inside dockerized container Used different models (RNN with LSTM, CNN, CNN with Word2Vec embedding layers) for training and stacking model for ensemble. Achieved 94% validation accuracy with ensemble model Classification of Extended MNIST using Persistent Homology EMNIST dataset is MNIST (handwritten digit) dataset with handwritten characters Applied Persistent Homology and Principal Component Analysis to reduce the dimensionality of dataset. Reduced feature size from 784 (28x28) to 35 while retaining 99% variance Utilized libraries giotto-ai along-side standard deep learning libraries sklearn, NumPy, Tensorflow/Keras Achieved 97%-91% training-testing accuracy with a shallow neural network with only 3 hidden layers Utilization of CNN in speech recognition Classify Google Speech Command using Convolutional Neural Network (CNN) on audio data Created pipelines to process audio data to image features Audio data augmentation with respect to image features Used multiple CNN architectures (LeNet, MiniGoogleNet, AlexNet) for training and stacking model for ensemble. Achieved 91% validation accuracy with ensemble model. ","permalink":"https://n0k0m3.github.io/projects/1_deep_learning/","summary":"\u003ch3 id=\"deep-transformer-soft-actor-critic-network-for-reinforcement-learning\"\u003e\u003ca href=\"https://github.com/sesem738/Frankenstein\"\u003eDeep Transformer Soft Actor-Critic Network for Reinforcement Learning\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUtilize Transformer as memory module for both Actor and Policy networks\u003c/li\u003e\n\u003cli\u003eHyperparameter tuning for SAC performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"sentiment-analysis-on-myanimelist-user-ratings\"\u003e\u003ca href=\"https://github.com/n0k0m3/rnn-mal-sentiment\"\u003eSentiment Analysis on MyAnimeList User Ratings\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMyAnimeList is a popular anime rating website.\u003c/li\u003e\n\u003cli\u003ePredict user rating based on review using Recurrent Neural Network (RNN)\u003c/li\u003e\n\u003cli\u003eSetup a data-mining pipeline utilizing self-hosted REST API with a Redis server for caching inside dockerized container\u003c/li\u003e\n\u003cli\u003eUsed different models (RNN with LSTM, CNN, CNN with Word2Vec embedding layers) for training and stacking model for ensemble.\u003c/li\u003e\n\u003cli\u003eAchieved 94% validation accuracy with ensemble model\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"classification-of-extended-mnist-using-persistent-homology\"\u003e\u003ca href=\"https://colab.research.google.com/drive/18z161k3diYO6sNVBfiKH8uGqbrekxMPN?usp=sharing#scrollTo=0Y6rquBvdjEG\"\u003eClassification of Extended MNIST using Persistent Homology\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEMNIST dataset is MNIST (handwritten digit) dataset with handwritten \u003cstrong\u003echaracters\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eApplied Persistent Homology and Principal Component Analysis to reduce the dimensionality of dataset. Reduced feature size from 784 (28x28) to 35 while retaining 99% variance\u003c/li\u003e\n\u003cli\u003eUtilized libraries \u003ccode\u003egiotto-ai\u003c/code\u003e along-side standard deep learning libraries \u003ccode\u003esklearn\u003c/code\u003e, \u003ccode\u003eNumPy\u003c/code\u003e, \u003ccode\u003eTensorflow/Keras\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eAchieved 97%-91% training-testing accuracy with a shallow neural network with only 3 hidden layers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"utilization-of-cnn-in-speech-recognition\"\u003e\u003ca href=\"https://colab.research.google.com/drive/1KCJjwgW6VDlANLmXYTotatk2xux3nw0N?usp=sharing\"\u003eUtilization of CNN in speech recognition\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClassify Google Speech Command using Convolutional Neural Network (CNN) on audio data\u003c/li\u003e\n\u003cli\u003eCreated pipelines to process audio data to image features\u003c/li\u003e\n\u003cli\u003eAudio data augmentation with respect to image features\u003c/li\u003e\n\u003cli\u003eUsed multiple CNN architectures (LeNet, MiniGoogleNet, AlexNet) for training and stacking model for ensemble.\u003c/li\u003e\n\u003cli\u003eAchieved 91% validation accuracy with ensemble model.\u003c/li\u003e\n\u003c/ul\u003e","title":"Deep Learning Projects"},{"content":"Analysis of ProtonDB Linux Distribution Analyze trends of distributions market share in Gaming segment, based on ProtonDB user reports. Visuals to demonstrate the impact of Steam Deck release on Linux distribution market share. Spotify API Audio Feature Analysis From audio data predict track\u0026rsquo;s attribute, reverse engineer/analyze audio features of Spotify API. A (close to) comprehensive analysis of Spotify API Audio Features. Using datamined audio samples, convert to image representation of audio data. Use image representation to predict Spotify audio features. ","permalink":"https://n0k0m3.github.io/projects/2_data_analysis/","summary":"\u003ch3 id=\"analysis-of-protondb-linux-distribution\"\u003e\u003ca href=\"/posts/small-projects/protondb_analysis/\"\u003eAnalysis of ProtonDB Linux Distribution\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAnalyze trends of distributions market share in Gaming segment, based on ProtonDB user reports.\u003c/li\u003e\n\u003cli\u003eVisuals to demonstrate the impact of Steam Deck release on Linux distribution market share.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"spotify-api-audio-feature-analysis\"\u003e\u003ca href=\"https://github.com/n0k0m3/Spotify-API-Audio-Feature-Analysis\"\u003eSpotify API Audio Feature Analysis\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFrom \u003cstrong\u003eaudio data\u003c/strong\u003e predict track\u0026rsquo;s attribute, reverse engineer/analyze audio features of Spotify API.\u003c/li\u003e\n\u003cli\u003eA (close to) comprehensive analysis of Spotify API Audio Features.\u003c/li\u003e\n\u003cli\u003eUsing datamined \u003cstrong\u003eaudio samples\u003c/strong\u003e, convert to image representation of audio data.\u003c/li\u003e\n\u003cli\u003eUse image representation to predict Spotify audio features.\u003c/li\u003e\n\u003c/ul\u003e","title":"Data Analysis Projects"},{"content":"Jupyter Notebook Docker with Spark and DeltaLake support Attempts to replicate Databricks Runtime, plus features from feature-rich jupyter/docker-stacks. Based image on NVIDIA\u0026rsquo;s rapidsai/rapidsai image. Support for Spark/PySpark 3.2.x and Delta Lake 1.1.0. Monthly cronjob to update the image with latest features from upstream jupyter/docker-stacks CD/CI automate building of image and pushing to DockerHub and ghcr.io Docker container for Data Science: Based on Jupyter docker-stack jupyter/datascience-notebook ","permalink":"https://n0k0m3.github.io/projects/3_devops/","summary":"\u003ch3 id=\"jupyter-notebook-docker-with-spark-and-deltalake-support\"\u003e\u003ca href=\"https://github.com/n0k0m3/pyspark-notebook-deltalake-docker\"\u003eJupyter Notebook Docker with Spark and DeltaLake support\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAttempts to replicate \u003ccode\u003eDatabricks Runtime\u003c/code\u003e, plus features from feature-rich jupyter/docker-stacks.\u003c/li\u003e\n\u003cli\u003eBased image on NVIDIA\u0026rsquo;s \u003ccode\u003erapidsai/rapidsai\u003c/code\u003e image.\u003c/li\u003e\n\u003cli\u003eSupport for \u003ca href=\"https://spark.apache.org/downloads.html\"\u003eSpark\u003c/a\u003e/\u003ca href=\"https://spark.apache.org/docs/latest/api/python/\"\u003ePySpark\u003c/a\u003e 3.2.x and \u003ca href=\"https://delta.io/\"\u003eDelta Lake\u003c/a\u003e 1.1.0.\u003c/li\u003e\n\u003cli\u003eMonthly cronjob to update the image with latest features from upstream \u003ccode\u003ejupyter/docker-stacks\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCD/CI automate building of image and pushing to \u003ccode\u003eDockerHub\u003c/code\u003e and \u003ccode\u003eghcr.io\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"docker-container-for-data-science\"\u003e\u003ca href=\"https://github.com/n0k0m3/datascience-notebook-docker\"\u003eDocker container for Data Science\u003c/a\u003e:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBased on Jupyter docker-stack \u003ccode\u003ejupyter/datascience-notebook\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"MLOps/Data Science DevOps Projects"},{"content":"Date A Live: Spirit Pledge Game Analysis Assets Decryption Tool: Reverse Engineer mobile game Date A Live: Spirit Pledge using Static analysis tool from NSA ghidra and dynamic analysis tool frida. Re-implement decryption functions using Python, implement methods to convert PowerVR, Ericsson Texture Compression format to digital images format (JPEG/PNG) Assets Mining CD/CI: - Data-mined source logics to find insecure API/server that allows easy download/extraction of new game contents. - Datamining repository above developed decryption tool. Using cronjob and Github Action to automate fetch, decrypt and datamine new contents. Usable mined data examples: Extract Live2D assets compatible with Live2DViewerEX: Link Dating Route and Favorites: Link Other games reverse engineering/analysis Azur Lane (Autopatcher): Reverse engineer Azur Lane game client and edit (patch) the game logic automatically. Arknights Assets Decryption: Decrypt Arknights game assets by extracting AES encryption key via dynamic analysis (Frida). ","permalink":"https://n0k0m3.github.io/projects/4_game_re/","summary":"\u003ch3 id=\"date-a-live-spirit-pledge-game-analysis\"\u003eDate A Live: Spirit Pledge Game Analysis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/DALSP-Assets-Decryption-tool\"\u003eAssets Decryption Tool\u003c/a\u003e:\n\u003cul\u003e\n\u003cli\u003eReverse Engineer mobile game Date A Live: Spirit Pledge using Static analysis tool from NSA ghidra and dynamic analysis tool frida.\u003c/li\u003e\n\u003cli\u003eRe-implement decryption functions using Python, implement methods to convert PowerVR, Ericsson Texture Compression format to digital images format (JPEG/PNG)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/DateALiveData\"\u003eAssets Mining CD/CI\u003c/a\u003e: - Data-mined source logics to find insecure API/server that allows easy download/extraction of new game contents. - Datamining repository above developed decryption tool. Using cronjob\nand Github Action to automate fetch, decrypt and datamine new contents.\u003c/li\u003e\n\u003cli\u003eUsable mined data examples:\n\u003cul\u003e\n\u003cli\u003eExtract Live2D assets compatible with Live2DViewerEX: \u003ca href=\"https://github.com/n0k0m3/DALSP-Live2D\"\u003eLink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDating Route and Favorites: \u003ca href=\"https://github.com/n0k0m3/DALSP-Dating-Routes-Dump\"\u003eLink\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"other-games-reverse-engineeringanalysis\"\u003eOther games reverse engineering/analysis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/Azur-Lane-Scripts-Autopatcher\"\u003eAzur Lane (Autopatcher)\u003c/a\u003e: Reverse engineer Azur Lane game client and edit (patch) the game logic automatically.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/Arknights-Lua-Decrypter\"\u003eArknights Assets Decryption\u003c/a\u003e: Decrypt Arknights game assets by extracting AES encryption key via dynamic analysis (Frida).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- - [D4DJ Assets Decryption](https://github.com/n0k0m3/D4DJ) --\u003e","title":"Games Reverse Engineering and Data Mining Projects"},{"content":"Vaultwarden on Cloudflare A turn-key deployment for self-hosting Bitwarden using Cloudflare Tunnel.\nThis is very useful for people who want to self-host Bitwarden but don\u0026rsquo;t have a static IP address. With the recent attacks on LastPass and other password manager providers, it\u0026rsquo;s time to take control of your own data. WandB self-hosting license generator For education purpose only, support generating license for self-hosting WandB server.\nDocker Compose for Docker-OSX Quick docker-compose deployment to run macOS in docker environment for security research.\n","permalink":"https://n0k0m3.github.io/projects/5_self_hosting/","summary":"\u003ch3 id=\"vaultwarden-on-cloudflare\"\u003e\u003ca href=\"https://github.com/n0k0m3/vaultwarden_cloudflare\"\u003eVaultwarden on Cloudflare\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA turn-key deployment for self-hosting Bitwarden using Cloudflare Tunnel.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThis is very useful for people who want to self-host Bitwarden but don\u0026rsquo;t have a static IP address.\u003c/li\u003e\n\u003cli\u003eWith the recent attacks on LastPass and other password manager providers, it\u0026rsquo;s time to take control of your own data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"wandb-self-hosting-license-generator\"\u003e\u003ca href=\"https://github.com/n0k0m3/wandb_local_lic\"\u003eWandB self-hosting license generator\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFor education purpose only, support generating license for self-hosting WandB server.\u003c/p\u003e\n\u003ch3 id=\"docker-compose-for-docker-osx\"\u003e\u003ca href=\"https://github.com/n0k0m3/docker-osx-compose\"\u003eDocker Compose for Docker-OSX\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eQuick \u003ccode\u003edocker-compose\u003c/code\u003e deployment to run macOS in docker environment for security research.\u003c/p\u003e","title":"Self-hosting Projects"},{"content":"These repos contains all of my personal codes and guides for personal setups. Most scripts work with all common consumer-based distros (Debian/Ubuntu, Arch, maybe RHEL-based, Fedora for some)\nLibrary Genesis Torrent Scrapper: Scrapes torrents that need seeding for Library Genesis Project for preservation. Not intended for piracy Jpopsuki Torrent Scrapper: Scrapes small torrents for hoarding seed points on private music tracker Jpopsuki. Not intended for piracy pwned password checker: Check export BitWarden passwords against haveibeenpwned.com API. ReVanced Build Action: For education purpose only, support building ReVanced - a modded YouTube app for Android with a single click using Github Actions. ","permalink":"https://n0k0m3.github.io/projects/6_small_projects/","summary":"\u003cp\u003eThese repos contains all of my personal codes and guides for personal setups. Most scripts work with all common consumer-based distros (Debian/Ubuntu, Arch, maybe RHEL-based, Fedora for some)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/Personal-Setup/blob/main/Libgen_torrent_scrape/\"\u003eLibrary Genesis Torrent Scrapper\u003c/a\u003e: Scrapes torrents that need seeding for Library Genesis Project for preservation. Not intended for piracy\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/jpopsuki-scraping\"\u003eJpopsuki Torrent Scrapper\u003c/a\u003e: Scrapes small torrents for hoarding seed points on private music tracker Jpopsuki. Not intended for piracy\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/bitwarden-haveibeenpwned\"\u003epwned password checker\u003c/a\u003e: Check export BitWarden passwords against haveibeenpwned.com API.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/n0k0m3/revanced-build-template\"\u003eReVanced Build Action\u003c/a\u003e: For education purpose only, support building ReVanced - a modded YouTube app for Android with a single click using Github Actions.\u003c/li\u003e\n\u003c/ul\u003e","title":"Miscellaneous Small Projects"},{"content":"Introduction In this blog post, we will guide you through the process of setting up a baremetal Kubernetes cluster tailored specifically for the Neural Transmissions (NETS)1 lab, using NVIDIA DeepOps. JupyterHub, a multi-user server that manages and proxies multiple instances of the Jupyter notebook server, is a perfect fit for this research environment. It facilitates seamless collaboration and sharing of data science projects among researchers. Throughout this tutorial, we will walk you through the steps to set up a Kubernetes cluster on baremetal servers equipped with NVIDIA GPUs, and get it ready for the deployment of JupyterHub.\nPrerequisites Before we begin, ensure that you have the following:\nA few baremetal servers with NVIDIA GPUs (e.g., Tesla, A100, or V100) and Ubuntu 18.04 or later installed. A provision machine with Ubuntu 18.04 or later installed (can be one of the cluster node). SSH access to the servers and root privileges. NVIDIA GPU drivers installed on the servers. Basic knowledge of Kubernetes and JupyterHub. Setting up the Kubernetes Cluster with NVIDIA DeepOps For this deployment we patch DeepOps using this repo to deploy due to a configuration bug in Kubespray, which affects cert-manager and potentially other services to communicate via .svc and .cluster.local DNS names.\nStep 1: Install DeepOps First, let\u0026rsquo;s set up a provision machine, which will run DeepOps and Kubespray to deploy the Kubernetes cluster. You can use one of the cluster nodes or a separate machine running Ubuntu 18.04 or later (in this tutorial we\u0026rsquo;ll be using a cluster master node for provisioning). SSH into the provision machine and run the following commands:\ncd ~ git clone https://github.com/NVIDIA/deepops cd deepops ./scripts/setup.sh This will install Ansible other required dependencies on the provision machine. Now we will clone the NETS-deepops-patch repo and patch DeepOps:\ncd ~ git clone https://github.com/NEural-TransmissionS/NETS-deepops-patch cd NETS-deepops-patch ./patch.sh Next, we will configure the cluster.\nStep 2: Configure the cluster Configure inventory Edit config/inventory. Add the nodes to the inventory file, specifying their respective hostnames or IP addresses, like following:\n[all] # current provision node is master node # n0k0m3-master is localhost defined in /etc/hosts n0k0m3-master ansible_host=\u0026lt;node-ip\u0026gt; n0k0m3-node01 ansible_host=\u0026lt;node-ip\u0026gt; In the same file, configure cluster node configuration (master,etcd,worker):\n###### # KUBERNETES ###### [kube-master] n0k0m3-master # Odd number of nodes required [etcd] n0k0m3-master [kube-node] n0k0m3-master n0k0m3-node01 [k8s-cluster:children] kube-master kube-node Note: here we\u0026rsquo;re using the master node as both the master and worker node. etcd and master/control-plane node are usually the same.\nConfigure storage By default, deepops setups an NFS server on the first kube-master node with export path /export/deepops_nfs for nfs-client-provisioner StorageClass. We will use this as a temporary storage for the cluster. Next part of this tutorial will offer a better solution for storage.\nOther DeepOps configurations The patch already includes some of the currently used configurations for NETS lab. You can edit the configurations in config/group_vars/all.yml and config/group_vars/k8s-cluster.yml to suit your needs. Most of the configurations are self-explanatory. For more information, refer to the DeepOps documentation\nStep 3: Deploy the cluster Now we are ready to deploy the cluster. Run the following command to deploy the cluster:\ncd ~/deepops ansible all -m raw -a \u0026#34;hostname\u0026#34; # verify configuration ansible-playbook -l k8s-cluster playbooks/k8s-cluster.yml Verify GPU nodes are ready:\nexport CLUSTER_VERIFY_EXPECTED_PODS=2 # Expected number of GPUs in the cluster ./scripts/k8s/verify_gpu.sh Step 4: Configure kubectl locally At this point, the cluster is ready to use. However, we need to configure kubectl locally to access the cluster. Copy ~/.kube/config from the provision machine to the same path on your local machine (using scp or any file transfer software).\nWe will need to edit the server field in the config file to point to the master node\u0026rsquo;s external IP address. Open the config file and edit the server field in the cluster section:\napiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026lt;CA-CERT\u0026gt; server: https://\u0026lt;KUBEAPI-SERVER-EXTERNAL-IP\u0026gt;:6443 name: cluster.local contexts: If we try to run kubectl get nodes now, we will get an error:\n$ kubectl get nodes E0509 21:08:29.820093 8619 memcache.go:265] couldn\u0026#39;t get current server API group list: Get \u0026#34;https://\u0026lt;external-ip\u0026gt;:6443/api?timeout=32s\u0026#34;: tls: failed to verify certificate: x509: certificate is valid for \u0026lt;kube-local-ip\u0026gt;, \u0026lt;node-local-ip\u0026gt;, 127.0.0.1, not \u0026lt;external-ip\u0026gt; ... Unable to connect to the server: tls: failed to verify certificate: x509: certificate is valid for \u0026lt;kube-local-ip\u0026gt;, \u0026lt;node-local-ip\u0026gt;, 127.0.0.1, not \u0026lt;external-ip\u0026gt; This is because the external IP address of the master node is not included in the kubeadm certificate. To fix this, we need to add the IP to kubeadm-config:\nsudo nano /etc/kubernetes/kubeadm-config.yaml ... extraVolumes: - name: usr-share-ca-certificates hostPath: /usr/share/ca-certificates mountPath: /usr/share/ca-certificates readOnly: true certSANs: - kubernetes - kubernetes.default - kubernetes.default.svc - kubernetes.default.svc.cluster.local - \u0026lt;kube-local-ip\u0026gt; - localhost - 127.0.0.1 - n0k0m3-master - lb-apiserver.kubernetes.local - \u0026lt;node-local-ip\u0026gt; - \u0026lt;external-ip\u0026gt; # we add our external IP here timeoutForControlPlane: 5m0s controllerManager: extraArgs: node-monitor-grace-period: 40s ... Remove existing certificates for kube-apiserver and re-generate them:\nsudo rm /etc/kubernetes/pki/apiserver.{crt,key} sudo kubeadm init phase certs apiserver --config /etc/kubernetes/kubeadm-config.yaml We will also need to kill existing kube-apiserver pods to force them to restart:\nkubectl -n kube-system get pods -l component=kube-apiserver -o name | cut -d\u0026#39;/\u0026#39; -f2 | xargs -I{} kubectl -n kube-system delete pod {} Now we should be able to access the cluster from our local machine:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION n0k0m3-master Ready control-plane 41m v1.25.6 n0k0m3-node01 Ready \u0026lt;none\u0026gt; 40m v1.25.6 Conclusion We have successfully deployed a Kubernetes cluster on the NETS lab. In the next part of this tutorial, we will deploy a StorageClass using piraeus-operator to provide persistent storage for the cluster.\nThe Neural Transmissions (NETS) Lab is part of the Department of Mathematical Sciences at the Florida Institute of Technology. The lab focuses on developing deep learning models, explainable AI, traditional machine learning, and statistical analysis applied to various domains. For more information, visit https://research.fit.edu/nets/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://n0k0m3.github.io/posts/nets-deployment/nets-deployment-1/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn this blog post, we will guide you through the process of setting up a baremetal Kubernetes cluster tailored specifically for the Neural Transmissions (NETS)\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e lab, using NVIDIA DeepOps. JupyterHub, a multi-user server that manages and proxies multiple instances of the Jupyter notebook server, is a perfect fit for this research environment. It facilitates seamless collaboration and sharing of data science projects among researchers. Throughout this tutorial, we will walk you through the steps to set up a Kubernetes cluster on baremetal servers equipped with NVIDIA GPUs, and get it ready for the deployment of JupyterHub.\u003c/p\u003e","title":"JupyterHub on Baremetal Kubernetes - Part 1 - Setting up cluster"},{"content":"OBS Studio flickering with Intel Graphics With some Intel graphics cards on XOrg (in my case, Tiger Lake Xe), OBS Studio will flicker with screen capture.\nsudo nano /etc/X11/xorg.conf.d/20-intel.conf Section \u0026#34;Device\u0026#34; Identifier \u0026#34;Intel Graphics\u0026#34; Driver \u0026#34;modesetting\u0026#34; EndSection Reboot or restart XOrg server to apply the fix.\nDelete Goodix Fingerprint Sensor saved fingerprint data If you installed Windows and enrolled fingerprints with Goodix Fingerprint Sensor, the saved fingerprint data will prevent new enrollments in Linux.\nRequirements:\nlibfprint, fprintd for fingerprint sensor python3 to run the script Download the following script and run it (adapted from this issue but use the new clear_storage_sync() method instead of delete_print_sync() loop):\nDownload delete_goodix_fingerprint_data.py{: .btn .btn\u0026ndash;info }\nsudo python3 delete_goodix_fingerprint_data.py Other Solution\nAnother solution is provided by Devyn_Cairns from Framework Community (Link to the solution). The author provided an AppImage to run the script with all dependencies. From my testing it\u0026rsquo;s more stable than my script.\nsudo ./fprint-clear-storage-0.0.1-x86_64.AppImage ","permalink":"https://n0k0m3.github.io/posts/setting_up_arch/bug_fixes/","summary":"\u003ch2 id=\"obs-studio-flickering-with-intel-graphics\"\u003eOBS Studio flickering with Intel Graphics\u003c/h2\u003e\n\u003cp\u003eWith some Intel graphics cards on XOrg (in my case, Tiger Lake Xe), OBS Studio will flicker with screen capture.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo nano /etc/X11/xorg.conf.d/20-intel.conf\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSection \u0026#34;Device\u0026#34;\n    Identifier \u0026#34;Intel Graphics\u0026#34;\n    Driver \u0026#34;modesetting\u0026#34;\nEndSection\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eReboot or restart XOrg server to apply the fix.\u003c/p\u003e\n\u003ch2 id=\"delete-goodix-fingerprint-sensor-saved-fingerprint-data\"\u003eDelete Goodix Fingerprint Sensor saved fingerprint data\u003c/h2\u003e\n\u003cp\u003eIf you installed Windows and enrolled fingerprints with Goodix Fingerprint Sensor, the saved fingerprint data will prevent new enrollments in Linux.\u003c/p\u003e","title":"Setting up Arch - Part 5 - Bugs Fixes"},{"content":"Follow instructions from https://guide.13375384.xyz/start or https://vita.hacks.guide/.\nDocuments for compiling from sources for tools. Guide to mount TexFAT Note: Put device in airplane mode during the Content Manager connection process will make the steps much easier to do.\nDON\u0026rsquo;T MANUALLY SETTING YAMT, JUST USE YAMT SETUP USING VITADEPLOY (guide)\nFinalHE Check prerequisites (https://github.com/soarqin/finalhe)\ngit clone https://github.com/soarqin/finalhe cd finalhe qmake \u0026amp;\u0026amp; make Build artifacts in src/FinalHE. Copy VitaDeploy zip in the same folder as FinalHE binary\nYAMT TexFAT mount Install exfat-nofuse. [Arch AUR] [Other distros build from source]\nNote: Use rsync to copy files from host to sd2vita instead of Nautilus or other GUI File Manager.\nBuild exfat-nofuse from source git clone https://github.com/relan/exfat cd exfat git checkout v1.3.0 # Patch for `exfat-nofuse` sed -i \u0026#34;/fuse/d\u0026#34; configure.ac sed -i \u0026#34;s/ fuse//\u0026#34; Makefile.am # Build autoreconf -fiv ./configure --prefix=/usr --sbindir=/usr/bin make CCFLAGS=\u0026#34;${CFLAGS} ${CPPFLAGS} -std=c99\u0026#34; LINKFLAGS=\u0026#34;${LDFLAGS}\u0026#34; # Install sudo make install After installing exfat-nofuse: sudo modprobe exfat xdelta3 xdelta3 is a binary diff tool used by some modders (for example, MrComputerRevo on Grisaia Triology + Spin off patch).\nBinary of this library is only available for windows. Build steps are documented below.\ngit clone https://github.com/jmacd/xdelta cd xdelta/xdelta3 git checkout 4b4aed71a959fe11852e45242bb6524be85d3709 export CFLAGS=\u0026#34;$CFLAGS -w\u0026#34; export CXXFLAGS=\u0026#34;$CFLAGS -w\u0026#34; aclocal autoreconf --install libtoolize autoconf autoheader automake --add-missing automake ./configure --disable-dependency-tracking --prefix=/usr --with-liblzma make Build artifacts in xdelta/xdelta3/xdelta3\nNPS Browser and pkg2zip Use NPS Browser and pkg2zip with wine + wine-mono. You don\u0026rsquo;t really need wine-gecko for patch checking but if you REALLY want you can just install it.\n","permalink":"https://n0k0m3.github.io/posts/ps_vita_hacking/","summary":"\u003cp\u003eFollow instructions from \u003ca href=\"https://guide.13375384.xyz/start\"\u003ehttps://guide.13375384.xyz/start\u003c/a\u003e or \u003ca href=\"https://vita.hacks.guide/\"\u003ehttps://vita.hacks.guide/\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDocuments for compiling from sources for tools.\u003c/li\u003e\n\u003cli\u003eGuide to mount TexFAT\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Put device in airplane mode during the \u003ccode\u003eContent Manager\u003c/code\u003e connection process will make the steps much easier to do.\u003c/p\u003e\n\u003cp\u003eDON\u0026rsquo;T MANUALLY SETTING YAMT, JUST USE YAMT SETUP USING VITADEPLOY (\u003ca href=\"https://guide.13375384.xyz/start\"\u003eguide\u003c/a\u003e)\u003c/p\u003e\n\u003ch2 id=\"finalhe\"\u003eFinalHE\u003c/h2\u003e\n\u003cp\u003eCheck prerequisites (\u003ca href=\"https://github.com/soarqin/finalhe\"\u003ehttps://github.com/soarqin/finalhe\u003c/a\u003e)\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit clone https://github.com/soarqin/finalhe\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecd\u003c/span\u003e finalhe\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eqmake \u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e make\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBuild artifacts in \u003ccode\u003esrc/FinalHE\u003c/code\u003e. Copy \u003ccode\u003eVitaDeploy\u003c/code\u003e zip in the same folder as \u003ccode\u003eFinalHE\u003c/code\u003e binary\u003c/p\u003e","title":"Guide for hacking PSVita on Linux"},{"content":"fstab mount with user permssion Use KDE/GNOME partition manager to manage mount point and use mount options: uid=1000,gid=1001,umask=022\nOr append to /etc/fstab:\nUUID=\u0026lt;disk_UUID\u0026gt; \u0026lt;mount_point\u0026gt; \u0026lt;format\u0026gt; uid=1000,gid=1001,umask=022,noatime 0 1 fstab mount NTFS with kernel NTFS3 driver from kernel 5.15 Use KDE/GNOME partition manager to manage mount point, this will use the default mount.ntfs -\u0026gt; ntfs-3g driver. We need to change the mount driver to mount.ntfs3 in /etc/fstab:\nUUID=\u0026lt;disk_UUID\u0026gt; \u0026lt;mount_point\u0026gt; ntfs3 discard,noatime 0 0 ","permalink":"https://n0k0m3.github.io/posts/setting_up_arch/tips_and_tricks/","summary":"\u003ch2 id=\"fstab-mount-with-user-permssion\"\u003e\u003ccode\u003efstab\u003c/code\u003e mount with user permssion\u003c/h2\u003e\n\u003cp\u003eUse KDE/GNOME partition manager to manage mount point and use mount options: \u003ccode\u003euid=1000,gid=1001,umask=022\u003c/code\u003e\u003cbr\u003e\nOr append to \u003ccode\u003e/etc/fstab\u003c/code\u003e:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eUUID=\u0026lt;disk_UUID\u0026gt;   \u0026lt;mount_point\u0026gt;    \u0026lt;format\u0026gt;    uid=1000,gid=1001,umask=022,noatime 0 1\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"fstab-mount-ntfs-with-kernel-ntfs3-driver-from-kernel-515\"\u003e\u003ccode\u003efstab\u003c/code\u003e mount NTFS with kernel \u003ccode\u003eNTFS3\u003c/code\u003e driver from kernel 5.15\u003c/h2\u003e\n\u003cp\u003eUse KDE/GNOME partition manager to manage mount point, this will use the default \u003ccode\u003emount.ntfs -\u0026gt; ntfs-3g\u003c/code\u003e driver. We need to change the mount driver to \u003ccode\u003emount.ntfs3\u003c/code\u003e in \u003ccode\u003e/etc/fstab\u003c/code\u003e:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eUUID=\u0026lt;disk_UUID\u0026gt;   \u0026lt;mount_point\u0026gt;    ntfs3       discard,noatime                     0 0\n\u003c/code\u003e\u003c/pre\u003e","title":"Setting up Arch - Part 4 - Tips and Tricks"},{"content":"Install zsh4humans I don\u0026rsquo;t need anything more fancy than romkatv\u0026rsquo;s zsh4humans\nsh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/Setting_up_Arch/setup.sh)\u0026#34; Install Nextcloud sudo pacman -S nextcloud-client Sync .dotfiles with Nextcloud + flameshot shortcut\nSetting up pacman and makepkg config sudo nano /etc/pacman.conf Set ParallelDownloads = 10:\nsudo nano /etc/makepkg.conf Set compile flags CFLAGS=\u0026quot;-march=native -mtune=native ...\u0026quot; and MAKEFLAGS=\u0026quot;-j12\u0026quot; for 12 being total number of available cores/threads:\nComparing current installation with installed packages of previous dist python3 read_install.py \u0026lt;installed.log file\u0026gt; Update installed.log with current setup Download export_install_deps.py{: .btn .btn\u0026ndash;info }\npython export_install_deps.py --installed -q \u0026gt; installed.log ","permalink":"https://n0k0m3.github.io/posts/setting_up_arch/post_install/","summary":"\u003ch2 id=\"install-zsh4humans\"\u003eInstall \u003ca href=\"https://github.com/romkatv/zsh4humans\"\u003e\u003ccode\u003ezsh4humans\u003c/code\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eI don\u0026rsquo;t need anything more fancy than romkatv\u0026rsquo;s \u003ccode\u003ezsh4humans\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esh -c \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"k\"\u003e$(\u003c/span\u003ecurl -fsSL https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/Setting_up_Arch/setup.sh\u003cspan class=\"k\"\u003e)\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"install-nextcloud\"\u003eInstall Nextcloud\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo pacman -S nextcloud-client\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSync \u003ccode\u003e.dotfiles\u003c/code\u003e with Nextcloud + \u003ccode\u003eflameshot\u003c/code\u003e shortcut\u003c/p\u003e\n\u003ch2 id=\"setting-up-pacman-and-makepkg-config\"\u003eSetting up \u003ccode\u003epacman\u003c/code\u003e and \u003ccode\u003emakepkg\u003c/code\u003e config\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo nano /etc/pacman.conf\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSet \u003ccode\u003eParallelDownloads = 10\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo nano /etc/makepkg.conf\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSet compile flags \u003ccode\u003eCFLAGS=\u0026quot;-march=native -mtune=native ...\u0026quot;\u003c/code\u003e and \u003ccode\u003eMAKEFLAGS=\u0026quot;-j12\u0026quot;\u003c/code\u003e for \u003ccode\u003e12\u003c/code\u003e being total number of available cores/threads:\u003c/p\u003e\n\u003ch2 id=\"comparing-current-installation-with-installed-packages-of-previous-dist\"\u003eComparing current installation with installed packages of previous dist\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epython3 read_install.py \u0026lt;installed.log file\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"update-installedlog-with-current-setup\"\u003eUpdate \u003ccode\u003einstalled.log\u003c/code\u003e with current setup\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/Setting_up_Arch/export_install_deps.py\"\u003eDownload export_install_deps.py\u003c/a\u003e{: .btn .btn\u0026ndash;info }\u003c/p\u003e","title":"Setting up Arch - Part 3 - Post-Installation Setup"},{"content":"By default Arch-based distros uses busybox init, which doesn’t support some features comfort from systemd. This guide will help you to setup systemd hooks, switch encryption to LUKS2 for systemd-cryptenroll, use U2F/FIDO2 key to unlock at boot, and Plymouth for boot splash screen.\nsystemd hooks and Plymouth boot splash screen Follwing script will change busybox init to systemd hooks and setup plymouth with colorful_loop theme.\nRequirements: yay (default on EndeavourOS)\n# :: Install plymouth :: # yay -S --noconfirm --needed plymouth-git plymouth-theme-colorful-loop-git # :: systemd setup script :: # sudo sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/Setting_up_Arch/systemd_setup.sh)\u0026#34; LUKS2 encryption with U2F/FIDO2 key unlock Requires systemd hooks with sd-encrypt.\nBoot from USB/ISO of Arch/EndeavourOS. We don\u0026rsquo;t need chroot. Following script will convert LUKS1 to LUKS2.\n# :: Convert LUKS1 to LUKS2 :: # sudo su - lukspart=($(sudo blkid | grep crypto_LUKS)) lukspart=($(sed -r \u0026#34;s/(.*):/\\1/\u0026#34; \u0026lt;\u0026lt;\u0026lt; ${lukspart[0]})) luksversion=$(sudo cryptsetup luksDump $lukspart | grep Version) if grep -q \u0026#34;1\u0026#34; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$luksversion\u0026#34;; then echo \u0026#34;Converting LUKS1 to LUKS2\u0026#34; cryptsetup convert --type luks2 $lukspart elif grep -q \u0026#34;2\u0026#34; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$luksversion\u0026#34;; then echo \u0026#34;Partition is LUKS2 already\u0026#34; else echo \u0026#34;Unknown LUKS version\u0026#34; exit 1 fi (Optional) Add new secure passphrase if needed.\nsudo su - lukspart=($(sudo blkid | grep crypto_LUKS)) lukspart=($(sed -r \u0026#34;s/(.*):/\\1/\u0026#34; \u0026lt;\u0026lt;\u0026lt; ${lukspart[0]})) cryptsetup luksAddKey $lukspart # enter existing passphrase then new passphrase with confirmation cryptsetup luksKillSlot $lukspart 0 # remove old passphrase, enter new passphrase Setup U2F/FIDO2 key unlock.\n# :: Install libfido2 :: # sudo pacman -S --noconfirm --needed libfido2 # :: Configure luks2 decryption with FIDO2 using systemd-cryptenroll :: # sudo sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/Setting_up_Arch/fido2_luks_setup.sh)\u0026#34; This will use hmac-secret extension of FIDO2 protocol. This method is compatible with almost all FIDO2 devices (I\u0026rsquo;m using Yubico Security Key (Blue Key) as I can just use a single U2F key to unlock all OpenPGP, smart card, and OTP keys instead of storing them).\n","permalink":"https://n0k0m3.github.io/posts/setting_up_arch/systemd_setup/systemd_setup/","summary":"\u003cp\u003eBy default Arch-based distros uses \u003ccode\u003ebusybox\u003c/code\u003e init, which doesn’t support some features comfort from \u003ccode\u003esystemd\u003c/code\u003e. This guide will help you to setup \u003ccode\u003esystemd\u003c/code\u003e hooks, switch encryption to LUKS2 for \u003ccode\u003esystemd-cryptenroll\u003c/code\u003e, use U2F/FIDO2 key to unlock at boot, and \u003ccode\u003ePlymouth\u003c/code\u003e for boot splash screen.\u003c/p\u003e\n\u003ch2 id=\"systemd-hooks-and-plymouth-boot-splash-screen\"\u003e\u003ccode\u003esystemd\u003c/code\u003e hooks and Plymouth boot splash screen\u003c/h2\u003e\n\u003cp\u003eFollwing script will change \u003ccode\u003ebusybox\u003c/code\u003e init to \u003ccode\u003esystemd\u003c/code\u003e hooks and setup \u003ccode\u003eplymouth\u003c/code\u003e with \u003ccode\u003ecolorful_loop\u003c/code\u003e theme.\u003c/p\u003e\n\u003cp\u003eRequirements: \u003ccode\u003eyay\u003c/code\u003e (default on EndeavourOS)\u003c/p\u003e","title":"Setting up Arch - Part 2 - `systemd` hooks with FIDO2 unlock"},{"content":"This is setup on EndeavourOS/Manjaro, on barebone Arch should be a little bit different (install yay before all of these steps)\nArch setup Setting up Manjaro/EndeavourOS with Calamares installer using this partition Setup with SWAP file (turn on hibernation option):\nPartition Mount Point Filesystem Size Encryption Status EFI system partition /boot/efi FAT32 300-550 MB Unencrypted /boot partition /boot/efi ext4 200-500 MB Unencrypted root partition / btrfs/LUKS Rest of space Encrypted Note: mount EFI with boot flag and / with root flag.\nHide GRUB at boot $ sudo nano /etc/default/grub GRUB_DEFAULT=0 GRUB_TIMEOUT=1 GRUB_TIMEOUT_STYLE=hidden Add swap after installation As of the latest update, EndeavourOS/Manjaro now use latest Calamares installer (resolved this tracked issue), which supports swap file on encrypted btrfs natively. The below steps are for reference only.\nRequires `gcc`, `wget` to be installed (default on EndeavourOS) Just need to run:\nsudo sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/Setting_up_Arch/swap_setup.sh)\u0026#34; And Done! References https://discovery.endeavouros.com/btrfs/btrfs-resume-and-hibernate-setup/2021/12/ https://discovery.endeavouros.com/encrypted-installation/btrfsonluks-quick-copy-paste-version/2021/03/ https://discovery.endeavouros.com/storage-and-partitions/adding-swap-after-installation/2021/03/ ","permalink":"https://n0k0m3.github.io/posts/setting_up_arch/setting_up_arch/","summary":"\u003cp\u003eThis is setup on EndeavourOS/Manjaro, on barebone Arch should be a little bit different (install \u003ccode\u003eyay\u003c/code\u003e before all of these steps)\u003c/p\u003e\n\u003ch2 id=\"arch-setup\"\u003eArch setup\u003c/h2\u003e\n\u003cp\u003eSetting up Manjaro/EndeavourOS with Calamares installer using this partition Setup with SWAP file (turn on hibernation option):\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003ePartition\u003c/th\u003e\n          \u003cth\u003eMount Point\u003c/th\u003e\n          \u003cth\u003eFilesystem\u003c/th\u003e\n          \u003cth\u003eSize\u003c/th\u003e\n          \u003cth\u003eEncryption Status\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEFI system partition\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003e/boot/efi\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eFAT32\u003c/td\u003e\n          \u003ctd\u003e300-550 MB\u003c/td\u003e\n          \u003ctd\u003eUnencrypted\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e/boot\u003c/code\u003e partition\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003e/boot/efi\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eext4\u003c/td\u003e\n          \u003ctd\u003e200-500 MB\u003c/td\u003e\n          \u003ctd\u003eUnencrypted\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eroot\u003c/code\u003e partition\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003e/\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003ebtrfs/LUKS\u003c/td\u003e\n          \u003ctd\u003eRest of space\u003c/td\u003e\n          \u003ctd\u003eEncrypted\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNote: mount EFI with \u003ccode\u003eboot\u003c/code\u003e flag and \u003ccode\u003e/\u003c/code\u003e with \u003ccode\u003eroot\u003c/code\u003e flag.\u003c/p\u003e","title":"Setting up Arch - Part 1 - Swap on BTRFS"},{"content":"Data Preprocessing Notebook link\nCumulative Results Note: On Dec 2nd 2019, ProtonDB contribute workflow changed to a questionnaire, subsequently changed the data structures. All data prior to this date are for reference only and should NOT be inferred.\nRaw user counts per distro Normalized distro market share on ProtonDB Normalized distro market share on ProtonDB (Merged distro base) Arch Linux and Arch-based distro is on the rise, while Debian-based (Ubuntu) distros are on the decline.\nSteam Deck/SteamOS effect on ProtonDB market share analysis Is there a shift due to Steam Deck and SteamOS? We\u0026rsquo;ll filter from Jan 1st 2022 till now to see if there is a shift in the market share. Let\u0026rsquo;s group the distro by their bases. As we can see, the market share graph doesn\u0026rsquo;t show that there\u0026rsquo;s any effect of Steam Deck release on ProtonDB user base. Let\u0026rsquo;s check monthly increased user counts to see if there\u0026rsquo;s any change.\nMonthly delta of user counts Debian and Fedora experienced a spike in new user count. More analysis on the market is needed.\nDebian-based new installation is on a constant decline, while arch and \u0026ldquo;other\u0026rdquo; distros are on a rise. As the market share of \u0026ldquo;other\u0026rdquo; distros is not as significant as Debian or Arch derivatives, we won\u0026rsquo;t go deeper into this category.\nNew install of both Arch and Debian-based distros is on plateau. So for now, the release of Steam Deck is not that big of a disruption. Interestingly, by the end of March there is a spike in installation of Fedora due to a beta release of Fedora 36.\n","permalink":"https://n0k0m3.github.io/posts/small-projects/protondb_analysis/","summary":"\u003ch3 id=\"data-preprocessing\"\u003eData Preprocessing\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/n0k0m3/Personal-Setup/blob/main/ProtonDB_Analysis/analysis.ipynb\"\u003eNotebook link\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"cumulative-results\"\u003eCumulative Results\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e On Dec 2nd 2019, ProtonDB contribute workflow changed to a questionnaire, subsequently changed the data structures. All data prior to this date are for reference only and should NOT be inferred.\u003c/p\u003e\n\u003ch4 id=\"raw-user-counts-per-distro\"\u003eRaw user counts per distro\u003c/h4\u003e\n\n\n\u003cdiv id=\"protondb_data/protondb_user_count.json\" class=\"plotly\" style=\"height:600\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"protondb_data/protondb_user_count.json\", function(err, fig) {\n    Plotly.plot('protondb_data\\/protondb_user_count.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\n\n\u003ch4 id=\"normalized-distro-market-share-on-protondb\"\u003eNormalized distro market share on ProtonDB\u003c/h4\u003e\n\n\n\u003cdiv id=\"protondb_data/protondb_market_share.json\" class=\"plotly\" style=\"height:600\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"protondb_data/protondb_market_share.json\", function(err, fig) {\n    Plotly.plot('protondb_data\\/protondb_market_share.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\n\n\u003ch4 id=\"normalized-distro-market-share-on-protondb-merged-distro-base\"\u003eNormalized distro market share on ProtonDB (Merged distro base)\u003c/h4\u003e\n\u003cp\u003e\n\n\u003cdiv id=\"protondb_data/protondb_market_share_base.json\" class=\"plotly\" style=\"height:600\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"protondb_data/protondb_market_share_base.json\", function(err, fig) {\n    Plotly.plot('protondb_data\\/protondb_market_share_base.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\n\nArch Linux and Arch-based distro is on the rise, while Debian-based (Ubuntu) distros are on the decline.\u003c/p\u003e","title":"Analysis of ProtonDB Linux Distribution"},{"content":"Following guide was tested on EndeavourOS and Manjaro (Arch-based) Linux distro.\nWhy? Installing dependencies and setting up notebooks is usually a PITA: installing CUDA with CuDNN and TensorRT doesn\u0026rsquo;t have a common and easy to follow guide, along with recent (not recent) release of python 3.9, most ML/DL packages are not updated to this wheel. Also setting up venv is hard to maintain and migrate as you have to backup the whole environment to other machine.\ntensorflow/tensorflow docker container solves this problem by allowing user to backup personalized config, while don\u0026rsquo;t have to deal with maintaining the environment.\nRequirements Nvidia driver is installed (using nvidia-installer-dkms from EndeavourOS or from Arch repo, on Manjaro use their hardware configurator, for other distro DIY). Note: nouveau (default open-source NVIDIA driver) is not supported\nInstall docker Arch: (Arch Wiki guide)\nInstall docker\nsudo pacman -S docker Enable and start the service\nsudo systemctl enable --now docker.service (OPTIONAL, WARNING: insecure) Add your user to docker group in order to use docker command without sudo\nsudo groupadd docker sudo usermod -aG docker $USER Install nvidia-container toolkit for GPU access Install nvidia-container-toolkit(AUR)\nyay nvidia-container-toolkit Restart docker service\nsudo systemctl restart docker Check if GPU is available inside docker\ndocker run --gpus all nvidia/cuda:11.3.0-runtime-ubuntu20.04 nvidia-smi Remap user This will remap ALL users in docker to the current host-non-root (sudoer) user.\nFind userid and groupid for your user:\nid #example output: uid=1000(n0k0m3) gid=1001(n0k0m3) groups=1001(n0k0m3) We\u0026rsquo;ll use username=n0k0m3, uid=1000 and gid=1001 from here (default username and usergroup)\nsudo nano /etc/docker/daemon.json Add this json to the file\n{ \u0026#34;userns-remap\u0026#34;: \u0026#34;1000:1001\u0026#34; } Update subuid/gid:\n# Append sub-id to these files sudo touch /etc/subuid sudo touch /etc/subgid echo \u0026#34;n0k0m3:1000:65536\u0026#34; | sudo tee -a /etc/subuid echo \u0026#34;n0k0m3:1001:65536\u0026#34; | sudo tee -a /etc/subgid Finally restart docker daemon:\nsudo systemctl restart docker.service Run jupyter-tensorflow with port 6006 exposed for Tensorboard Change directory to path containing jupyter folder, then run:\nexport JUPYTER_PATH=$(realpath jupyter) docker run -d --gpus all \\ --name tf \\ -p 6006:6006 \\ -p 8888:8888 \\ -v $JUPYTER_PATH:/tf/notebooks \\ -v $JUPYTER_PATH/.jupyter:/root/.jupyter \\ -v $JUPYTER_PATH/.kaggle:/root/.kaggle \\ tensorflow/tensorflow:latest-gpu-jupyter You can change the build tag latest-gpu-jupyter to any other support build tag on DockerHub. However I\u0026rsquo;d suggest sticking with latest-gpu-jupyter or nightly-gpu-jupyter\nNext time to start the container:\ndocker start tf Common exposed ports setups tensorflow/tensorflow:latest-gpu-jupyter as tf for DL/AI training tasks 6006 - Tensorboard 8888 - JupyterLab notebook n0k0m3/pyspark-notebook-deltalake-docker as ds for PySpark + Deltalake support on jupyter/pyspark-notebook 8888 - JupyterLab notebook rapidsai/rapidsai:21.10-cuda11.4-runtime-ubuntu20.04-py3.8 as ra for GPU accelerated DataFrame 8888 - JupyterLab notebook 8786 - Dask scheduler 8787 - Dask diagnostic web server TODO docker-compose that pull image from hub, name container, exposes ports, and mount volumes paths Problem with Docker and BTRFS (copied from here) More than a problem is a caveat. If the main filesystem for root is BTRFS, docker will use BTRFS storage driver (Docker selects the storage driver automatically depending on the system\u0026rsquo;s configuration when it is installed) to create and manage all the docker images, layers and volumes. It is ok, but there is a problem with snapshots. Because /var/lib/docker is created to store all this stuff in a BTRFS subvolume which is into root subvolume, all this data won\u0026rsquo;t be included within the snapshots. In order to allow all this data be part of the snapshots, we can change the storage driver used by Docker. The preferred one is overlay2 right now. Please, check out this reference in order to select the proper storage driver for you. You must know that depending on the filesystem you have for root, some of the storage drivers will not be allowed.\nFor using overlay2:\nCreate a file called storage-driver.conf within /etc/systemd/system/docker.service.d/. If the directory doens\u0026rsquo;t exist, create the directory first. sudo mkdir -p /etc/systemd/system/docker.service.d/ sudo nano /etc/systemd/system/docker.service.d/storage-driver.conf This is the content of storage-driver.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --storage-driver=overlay2 Create /var/lib/docker/ and disable CoW (copy on write for BTRFS): sudo chattr +C /var/lib/docker Restart docker sudo systemctl restart docker.service ","permalink":"https://n0k0m3.github.io/posts/tensorflow_docker/","summary":"\u003cp\u003eFollowing guide was tested on EndeavourOS and Manjaro (Arch-based) Linux distro.\u003c/p\u003e\n\u003ch2 id=\"why\"\u003eWhy?\u003c/h2\u003e\n\u003cp\u003eInstalling dependencies and setting up notebooks is usually a PITA: installing CUDA with CuDNN and TensorRT doesn\u0026rsquo;t have a common and easy to follow guide, along with recent (not recent) release of python 3.9, most ML/DL packages are not updated to this wheel. Also setting up \u003ccode\u003evenv\u003c/code\u003e is hard to maintain and migrate as you have to backup the whole environment to other machine.\u003c/p\u003e","title":"Tensorflow with GPU support in Docker"},{"content":"Why single GPU passthrough? Because I\u0026rsquo;m poor, using Ryzen non-APU CPU, and I don\u0026rsquo;t want to buy a second GPU just for passthrough. I\u0026rsquo;m using a single GPU passthrough for gaming on Windows and using Linux as my daily driver.\nIn this guide, we will be going over how to set up a single GPU passthrough on Linux. This guide is meant to be a reference for myself and others who want to learn how to set up a single GPU passthrough on Linux. I\u0026rsquo;m using Arch Linux as the host OS, but this guide should work on any Linux distro.\n1. Notes Most of these commands will be run under root.\nsudo su - 2. Host Machine Settings 2.1 Enable \u0026amp; Verify IOMMU 2.1.1 Enabling IOMMU in BIOS Varies and depends on motherboard. Follow this guide: Arch Wiki\n2.1.2 Add kernel for IOMMU in GRUB Add these flags to the end of GRUB_CMDLINE_LINUX_DEFAULT variable\nnano /etc/default/grub For Intel CPU GRUB_CMDLINE_LINUX_DEFAULT=\u0026quot;... intel_iommu=on iommu=pt rd.driver.pre=vfio-pc\u0026quot; For AMD CPU GRUB_CMDLINE_LINUX_DEFAULT=\u0026quot;... amd_iommu=on iommu=pt rd.driver.pre=vfio-pc\u0026quot; Generate grub.cfg\ngrub-mkconfig -o /boot/grub/grub.cfg Reboot your system for the changes to take effect.\n2.1.3 Checking IOMMU By this point IOMMU should be enabled. Check if there\u0026rsquo;s a return\nsudo dmesg | grep \u0026#39;IOMMU\u0026#39; Example return\n[ 0.731687] pci 0000:00:00.2: AMD-Vi: IOMMU performance counters supported [ 0.732465] pci 0000:00:00.2: AMD-Vi: Found IOMMU cap 0x40 [ 0.732730] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank). [ 0.747591] AMD-Vi: AMD IOMMUv2 driver by Joerg Roedel \u0026lt;jroedel@suse.de\u0026gt; 2.1.4 IOMMU group IOMMU is the SMALLEST group that you can passthrough to VM, means that ALL devices in IOMMU must be passthrough to the VM\nRun this command to check IOMMU grouping:\nsh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/GPU_Passthrough/iommu.sh)\u0026#34; \u0026gt;\u0026gt; iommu.txt ## Delete \u0026#34;\u0026gt;\u0026gt; iommu.txt\u0026#34; to see output in stdout Example output iommu.txt\n... IOMMU group 23 09:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104 [GeForce RTX 3060 Ti] [10de:2486] (rev a1) [NORES]\t09:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:228b] (rev a1) ... Here our NVIDIA GPU is in group 23 and well isolated. Note the [NORES] flag means that the device won\u0026rsquo;t power-cycle properly after VM shutdown (Source). For now remember device IDs 09:00.0, 09:00.1 to passthrough\nIf your PCIe devices are not well-isolated, check ACS Override Kernel and ACS Patching Guide. However, I wouldn\u0026rsquo;t recommend this as it is really insecure (source). Better sell your motherboard and get one with better IOMMU grouping.\nInstall linux-zen (available as binary) or any linux-xanmod (build from source) kernel (or any kernel that have ACS override patches) and add pcie_acs_override=downstream,multifunction to the boot options in /etc/default/grub. I don\u0026rsquo;t need this for my system, but it\u0026rsquo;s good to know.\n2.2 Installing Packages Arch Linux\npacman -S qemu libvirt edk2-ovmf virt-manager dnsmasq ebtables Fedora dnf install @virtualization Ubuntu apt install qemu-kvm qemu-utils libvirt-daemon-system libvirt-clients bridge-utils virt-manager ovmf 2.3 Enable required services systemctl enable --now libvirtd Sometimes, you might need to start default network manually.\nvirsh net-start default virsh net-autostart default 3. Setup Virtual Machine I already have a VM with Windows 10 installed, with SSD passthrough, backed up with all definitions in virsh xml file so I\u0026rsquo;ll just import it.\nvirsh define \u0026lt;path-to-xml\u0026gt;/\u0026lt;vm-name\u0026gt;.xml Following is the guide to create a VM similar to my setup.\n3.1 Setting up VM and install Guest OS (Windows 10) NOTE: You should replace win10 with your VM\u0026rsquo;s name where applicable You should add your user to libvirt group to be able to run VM without root. And, input and kvm group for passing input devices.\nusermod -aG kvm,input,libvirt $USER Download virtio driver for windows. Launch virt-manager and create a new virtual machine. Select Customize before install on Final Step. In Overview section, set Chipset to Q35, and Firmware to UEFI using OVMF (Not tested with secboot so YMMV) In CPUs section, set CPU model to host-passthrough, and CPU Topology to whatever fits your system. For SATA disk of VM, set Disk Bus to virtio. In NIC section, set Device Model to virtio Add Hardware \u0026gt; CDROM: virtio-win.iso Now, Begin Installation. Windows can\u0026rsquo;t detect the virtio disk, so you need to Load Driver and select virtio-iso/amd64/win10 when prompted. After successful installation of Windows, install virtio drivers from virtio CDROM. You can then remove virtio iso. 3.2 Attaching PCI devices Keep Display Spice and Video QXL for debugging\nRemove Channel Spice, Sound ich* and other unnecessary devices.\nNow, click on Add Hardware, select PCI Host Device and add the PCI Host devices for your GPU\u0026rsquo;s VGA and HDMI Audio, in this example, [09:00.0] and [09:00.1]\n3.2.1 Video card driver virtualisation detection Spoof Hyper-V Vendor ID for GPU guest drivers (AMD).\n`virsh edit win10` ... \u0026lt;features\u0026gt; ... \u0026lt;hyperv\u0026gt; ... \u0026lt;vendor_id state=\u0026#39;on\u0026#39; value=\u0026#39;randomid\u0026#39;/\u0026gt; ... \u0026lt;/hyperv\u0026gt; ... \u0026lt;/features\u0026gt; ... NVIDIA guest drivers prior to version 465 require hiding the KVM CPU leaf (avoid error 43):\n`virsh edit win10` ... \u0026lt;features\u0026gt; ... \u0026lt;kvm\u0026gt; \u0026lt;hidden state=\u0026#39;on\u0026#39;/\u0026gt; \u0026lt;/kvm\u0026gt; ... \u0026lt;/features\u0026gt; ... 3.3 Keyboard/Mouse/Audio Passthrough I won\u0026rsquo;t be using passthrough as the latency and complicated setup is not worth it. Follow USB Controller Passthrough\n3.4 USB Controller Passthrough Passing through Audio through PulseAudio is laggy, and passing Keyboard/Mouse/Audio is complicated. Also, you won\u0026rsquo;t be able to use the main machine in Single GPU Passthrough setup anyway.\nRun this script to print which USB devices on which PCIe USB controller:\nsh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/n0k0m3/Personal-Setup/main/GPU_Passthrough/usb_iommu.sh)\u0026#34; \u0026gt;\u0026gt; usb_iommu.txt ## Delete \u0026#34;\u0026gt;\u0026gt; usb_iommu.txt\u0026#34; to see output in stdout Example output\n... Bus 5 --\u0026gt; 0000:0b:00.3 (IOMMU group 27) Bus 005 Device 005: ID 04d9:0024 Holtek Semiconductor, Inc. USB Gaming Keyboard Bus 005 Device 004: ID 046d:c53f Logitech, Inc. USB Receiver Bus 005 Device 003: ID 0fce:51e0 Sony Ericsson Mobile Communications AB F5122 [Xperia X dual] (developer mode) Bus 005 Device 002: ID 2109:2813 VIA Labs, Inc. VL813 Hub Bus 005 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 6 --\u0026gt; 0000:0b:00.3 (IOMMU group 27) Bus 006 Device 003: ID 2109:0813 VIA Labs, Inc. VL813 Hub Bus 006 Device 002: ID 1058:25fb Western Digital Technologies, Inc. easystore Desktop (WDBCKA) Bus 006 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub ... Note here that we don\u0026rsquo;t care about the bus but rather the PCIe hardware ID and IOMMU group. Let\u0026rsquo;s go back and check the grouping\n... IOMMU group 27 0b:00.3 USB controller [0c03]: Advanced Micro Devices, Inc. [AMD] Matisse USB 3.0 Host Controller [1022:149c] ... In my setup the USB controller is isolated with the rest, which is perfect. Now I just need to pass this whole device (0b:00.3) to VM\nTo find which USB port belongs to which controller/IOMMU group, replug your mouse/keyboard/random USB to every port on PC and rerun the script. Usually adjacent USB ports are from same controller.\nSome outlier (click to reveal) In my current setup Group 18 is also contains USB controller\n... IOMMU group 18 03:08.0 PCI bridge [0604]: Advanced Micro Devices, Inc. [AMD] Matisse PCIe GPP Bridge [1022:57a4] 06:00.0 Non-Essential Instrumentation [1300]: Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP [1022:1485] [NORES]\t06:00.1 USB controller [0c03]: Advanced Micro Devices, Inc. [AMD] Matisse USB 3.0 Host Controller [1022:149c] 06:00.3 USB controller [0c03]: Advanced Micro Devices, Inc. [AMD] Matisse USB 3.0 Host Controller [1022:149c] ... Corresponding USB bus\n... Bus 1 --\u0026gt; 0000:06:00.1 (IOMMU group 18) Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 2 --\u0026gt; 0000:06:00.1 (IOMMU group 18) Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 3 --\u0026gt; 0000:06:00.3 (IOMMU group 18) Bus 003 Device 005: ID 0b05:18f3 ASUSTek Computer, Inc. AURA LED Controller Bus 003 Device 003: ID 8087:0025 Intel Corp. Wireless-AC 9260 Bluetooth Adapter Bus 003 Device 008: ID 04d9:0024 Holtek Semiconductor, Inc. USB Gaming Keyboard Bus 003 Device 007: ID 0bda:4014 Realtek Semiconductor Corp. USB Audio Bus 003 Device 006: ID 0fce:51e0 Sony Ericsson Mobile Communications AB F5122 [Xperia X dual] (developer mode) Bus 003 Device 004: ID 046d:c53f Logitech, Inc. USB Receiver Bus 003 Device 002: ID 0424:2137 Microchip Technology, Inc. (formerly SMSC) USB2137B Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 4 --\u0026gt; 0000:06:00.3 (IOMMU group 18) Bus 004 Device 004: ID 1058:25fb Western Digital Technologies, Inc. easystore Desktop (WDBCKA) Bus 004 Device 003: ID 0bda:8153 Realtek Semiconductor Corp. RTL8153 Gigabit Ethernet Adapter Bus 004 Device 002: ID 0424:5537 Microchip Technology, Inc. (formerly SMSC) USB5537B Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub ... Due to reset problem, we need to figure out devices that can be power-cycled using linux. In group 18 one of our USB controller cannot be reset (06:00.1). Also LED controller, Wifi, etc. is bundled in this group, along with PCIe bridge, which is terrible to deal with and isolate. So we will not use this controller/controller group\n4. Libvirt Hooks Libvirt hooks automate the process of running specific tasks during VM state change. More info at: PassthroughPost\nCopy hooks folder to /etc/libvirt/\ncp -R hooks /etc/libvirt/ Edit /etc/libvirt/hooks/kvm.conf with regard to the output of iommu.txt\n5. vBIOS Patching (No need for my setup) NOTE: vBIOS patching is not patching directly into the hardware. You only patch the dumped ROM file. While most of the GPU can be passed with stock vBIOS, some GPU requires vBIOS patching depending on your host distro. In order to patch vBIOS, you need to first dump the GPU vBIOS from your system. If you have Windows installed, you can use GPU-Z to dump vBIOS. To dump vBIOS on Linux, you can use following command (replace PCI id with yours): If it doesn\u0026rsquo;t work on your distro, you can try using live cd.\necho 1 \u0026gt; /sys/bus/pci/devices/0000:01:00.0/rom cat /sys/bus/pci/devices/0000:01:00.0/rom \u0026gt; path/to/dump/vbios.rom echo 0 \u0026gt; /sys/bus/pci/devices/0000:01:00.0/rom To patch vBIOS, you need to use Hex Editor (eg., Okteta) and trim unnecessary header. For NVIDIA GPU, using hex editor, search string “VIDEO”, and remove everything before HEX value 55. For other GPU, I have no idea.\nTo use patched vBIOS, edit VM\u0026rsquo;s configuration to include patched vBIOS inside hostdev block of VGA\n`virsh edit win10` ```xml ... ... ... ... ``` References \u0026amp; See Also VFIO Single GPU Passthrough Configuration by Karuri\nSingle GPU Passthrough by joeknock90\nComplete Single GPU Passthrough by QaidVoid (format for this guide)\nArch Wiki PCI passthrough via OVMF\n","permalink":"https://n0k0m3.github.io/posts/single_gpu_passthrough_guide/","summary":"\u003cp\u003eWhy single GPU passthrough? Because I\u0026rsquo;m poor, using Ryzen non-APU CPU, and I don\u0026rsquo;t want to buy a second GPU just for passthrough. I\u0026rsquo;m using a single GPU passthrough for gaming on Windows and using Linux as my daily driver.\u003c/p\u003e\n\u003cp\u003eIn this guide, we will be going over how to set up a single GPU passthrough on Linux. This guide is meant to be a reference for myself and others who want to learn how to set up a single GPU passthrough on Linux. I\u0026rsquo;m using Arch Linux as the host OS, but this guide should work on any Linux distro.\u003c/p\u003e","title":"VFIO Single GPU Passthrough Guide on Linux"},{"content":"Personal setup stash This repo contains all of my personal codes and guides for personal setups. No personal info is here and most scripts work with all common consumer-based distros (Debian/Ubuntu, Arch, maybe RHEL-based, Fedora for some)\n","permalink":"https://n0k0m3.github.io/personal-setup/readme/","summary":"\u003ch1 id=\"personal-setup-stash\"\u003ePersonal setup stash\u003c/h1\u003e\n\u003cp\u003eThis repo contains all of my personal codes and guides for personal setups. No personal info is here and most scripts work with all common consumer-based distros (Debian/Ubuntu, Arch, maybe RHEL-based, Fedora for some)\u003c/p\u003e","title":""},{"content":"Hello, and welcome to my digital corner of curiosity and creation!\nI\u0026rsquo;m a Ph.D. student at Florida Tech, delving deep into the realms of Operations Research. My journey of learning is currently focused on the intriguing world of Electronic Health Records (EHRs) and the exciting potential they offer in Synthetic Data Generation.\nMy portfolio is a showcase of my endeavors, a collection of past and present projects that I\u0026rsquo;m incredibly proud of. For tech enthusiasts, my blog is a treasure trove of insights and explorations, where I share my experiences with Linux, Docker, VM, and Kubernetes.\nWhether you\u0026rsquo;re here out of curiosity, interest, or are considering bringing me on board, I invite you to view my resume.\nThank you for visiting, and enjoy your journey through my world of tech and innovation!\n","permalink":"https://n0k0m3.github.io/about/","summary":"\u003cp\u003eHello, and welcome to my digital corner of curiosity and creation!\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m a Ph.D. student at Florida Tech, delving deep into the realms of Operations Research. My journey of learning is currently focused on the intriguing world of Electronic Health Records (EHRs) and the exciting potential they offer in Synthetic Data Generation.\u003c/p\u003e\n\u003cp\u003eMy \u003ca href=\"/projects\"\u003eportfolio\u003c/a\u003e is a showcase of my endeavors, a collection of past and present projects that I\u0026rsquo;m incredibly proud of. For tech enthusiasts, my \u003ca href=\"/blog\"\u003eblog\u003c/a\u003e is a treasure trove of insights and explorations, where I share my experiences with Linux, Docker, VM, and Kubernetes.\u003c/p\u003e","title":"About Me"},{"content":"Privacy‑focused, longitudinal (temporal) generation of synthetic Electronics Health Records with Differential Privacy Electronic health records (EHRs) contain valuable information for medical research and analysis, but they also pose privacy risks for patients and providers. To address this challenge, this work aims to generate synthetic EHRs that preserve the features and patterns of real EHRs, including temporal features, while protecting the privacy of individuals.\nThe project involves four main steps:\nDeveloping a 3D representation of EHRs that captures the temporal and longitudinal aspects of patient data Using generative adversarial networks (GANs) with a privacy-preserving optimizer (using Differential Privacy) to generate realistic and diverse synthetic EHRs from the 3D representation Building a scalable pipeline to transform the synthetic EHRs into synthetic patient records that can be used for research and analysis Measuring the quality and utility of the synthetic data using various metrics, and assessing the privacy of the synthetic data using different privacy attack scenarios. Proposal/White Paper\nDownload PDF\nKeywords: electronic health records, synthetic data, generative adversarial networks, differential privacy, temporal features.\n3D Modeling of Resident Space Objects using Neural Radiance Fields Resident space objects (RSOs) are non-cooperative objects in orbit that pose challenges for space debris removal, on-orbit servicing (OOS), and functionality identification. This work presents a novel application of neural radiance fields (NeRFs) to the problem of 3D reconstruction of RSOs from a set of 2D images. NeRFs are a deep learning technique that can learn high-fidelity 3D models from unstructured images by modeling the scene as a continuous function that maps 3D coordinates and viewing directions to colors and densities.\nWe adapt two variants of NeRF, Instant NeRF and D-NeRF, to the RSO domain and evaluate their performance on datasets of images of a spacecraft mock-up captured under different lighting and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing and Navigation (ORION) Laboratory at Florida Institute of Technology. We show that Instant NeRF can produce realistic and detailed 3D models of RSOs with low computational cost, while D-NeRF can handle dynamic lighting changes and occlusions. We also discuss the potential applications of our approach for functionality identification and OOS assistance.\nPaper\n3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF\nKeywords: resident space objects, neural radiance fields, 3D reconstruction, functionality identification, on-orbit servicing.\nDetermination of Mutation Rates using Double Stochastic Process. Mutation rate is a key parameter in evolutionary biology and genetics, but it is difficult to estimate accurately. The most widely used method, Luria\u0026ndash;Delbrück method, relies on a simplifying assumption that the number of new mutations in each time step depends only on the total population of bacteria in the previous time step. However, this assumption can lead to errors and biases, due to the inclusion of existing mutants and new mutants from previous time steps. While some can argue that the number of existing mutants and new mutants from previous time frame is negligible, we showed that this number can converge to infinity even with very small mutation rate.\nIn this paper, we proposed a stochastic estimator that cover these issues of Luria\u0026ndash;Delbrück\u0026rsquo;s method, while also rigorously proved the convergence of the estimator in both probability and in L2-space, and validate the model empirically with experiments. Also, we expanded our estimator for the assumption that a microorganism can mutate or turn to 2 variants (which is a precursor to future research of an estimator for any finite n-variants), and showed that the estimator is also converge in probability, L2, and unbiased.\nPublication\nDetermination of Mutation Rates with Two Symmetric and Asymmetric Mutation Types\nKeywords: mutation rate, Luria\u0026ndash;Delbrück method, stochastic estimator, convergence analysis, empirical validation.\nReferences Caruso, Basilio, Trupti Mahendrakar, Van Minh Nguyen, Ryan T. White, and Todd Steffen. 2023. \u0026ldquo;3D Reconstruction of Non-Cooperative Resident Space Objects Using Instant NGP-Accelerated NeRF and d-NeRF.\u0026rdquo; https://arxiv.org/abs/2301.09060.\nDshalalow, Jewgeni H., Van Minh Nguyen, Richard R. Sinden, and Ryan T. White. 2022. \u0026ldquo;Determination of Mutation Rates with Two Symmetric and Asymmetric Mutation Types.\u0026rdquo; Symmetry 14 (8). https://doi.org/10.3390/sym14081701.\nTikayat Ray, Archana, Anirudh Prabhakara Bhat, Ryan T. White, Van Minh Nguyen, Olivia J. Pinon Fischer, and Dimitri N. Mavris. 2023. \u0026ldquo;Examining the Potential of Generative Language Models for Aviation Safety Analysis: Case Study and Insights Using the Aviation Safety Reporting System (ASRS).\u0026rdquo; Aerospace 10 (9). https://doi.org/10.3390/aerospace10090770.\n","permalink":"https://n0k0m3.github.io/research/","summary":"\u003ch2 id=\"privacyfocused-longitudinal-temporal-generation-of-synthetic-electronics-health-records-with-differential-privacy\"\u003ePrivacy‑focused, longitudinal (temporal) generation of synthetic Electronics Health Records with Differential Privacy\u003c/h2\u003e\n\u003cp\u003eElectronic health records (EHRs) contain valuable information for medical research and analysis, but they also pose privacy risks for patients and providers. To address this challenge, this work aims to generate synthetic EHRs that preserve the features and patterns of real EHRs, including temporal features, while protecting the privacy of individuals.\u003c/p\u003e\n\u003cp\u003eThe project involves four main steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDeveloping a 3D representation of EHRs that captures the temporal and longitudinal aspects of patient data\u003c/li\u003e\n\u003cli\u003eUsing generative adversarial networks (GANs) with a privacy-preserving optimizer (using Differential Privacy) to generate realistic and diverse synthetic EHRs from the 3D representation\u003c/li\u003e\n\u003cli\u003eBuilding a scalable pipeline to transform the synthetic EHRs into synthetic patient records that can be used for research and analysis\u003c/li\u003e\n\u003cli\u003eMeasuring the quality and utility of the synthetic data using various metrics, and assessing the privacy of the synthetic data using different privacy attack scenarios.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eProposal/White Paper\u003c/strong\u003e\u003c/p\u003e","title":"Researches"},{"content":"Here you can view and download my latest Resume\n","permalink":"https://n0k0m3.github.io/resume/","summary":"\u003cp\u003eHere you can view and download my latest Resume\u003c/p\u003e\n \n\n\u003cembed src=\"resume.pdf\" width=\"100%\" height=\"1000px\" type=\"application/pdf\" /\u003e","title":"Resume"},{"content":"Download Notebook{: .btn .btn\u0026ndash;info }\nNote: All graphs and plots are interactive. Feel free to zoom, pan, and edit the graphs for more granular details.\nQuestion 1 Part A Code import pandas as pd import plotly.express as px px.defaults.width = 600 px.defaults.height = 400 A quick view (first 5 rows) of the data\nCode data = pd.read_csv(\u0026#34;https://docs.google.com/spreadsheets/d/16i38oonuX1y1g7C_UAmiK9GkY7cS-64DfiDMNiR41LM/edit#gid=0\u0026#34;.replace(\u0026#39;/edit#gid=\u0026#39;, \u0026#39;/export?format=csv\u0026amp;gid=\u0026#39;)) data.head() order_id shop_id user_id order_amount total_items payment_method created_at 0 1 53 746 224 2 cash 2017-03-13 12:36:56 1 2 92 925 90 1 cash 2017-03-03 17:38:52 2 3 44 861 144 1 cash 2017-03-14 4:23:56 3 4 18 935 156 1 credit_card 2017-03-26 12:43:37 4 5 18 883 156 1 credit_card 2017-03-01 4:35:11 Code data.created_at = pd.to_datetime(data.created_at) data = data.sort_values([\u0026#34;created_at\u0026#34;]) Analysis 1: Order Amount Distribution Analysis Assuming zero-knowledge of the data view, let\u0026rsquo;s graph the order_amount distribution.\nCode fig = px.histogram(data.order_amount,title=\u0026#34;Order Amount Histogram\u0026#34;) fig.show() fig.write_json(\u0026#34;plot_data/order_amount_histogram.json\u0026#34;) We can see that the almost all order value is less than or equal $5000, let\u0026rsquo;s graph the data with this bound.\nCode fig = px.histogram(data.order_amount[data.order_amount\u0026lt;=5000],title=\u0026#34;Order Amount (with Amount \u0026lt; $5000) Histogram\u0026#34;) fig.show() fig.write_json(\u0026#34;plot_data/order_amount_histogram_5000.json\u0026#34;) We see that the distribution are left skewed and also bimodal: [0,220) represents first model and (220,5000+) represents the second model (From the shape of distribution we can assume the first one is normal distribution, the second model is Weibull/Gamma distribution. However, this assumption won\u0026rsquo;t play any role in the solutions).\nThe bimodal and skewed distribution, along with outlier points tell us that arithmetic mean of order amount is not a good indicator of the average order amount (AOV). As such, other metrics should be used to analyze the order amount average.\nAlso, we can see that the distribution is mostly concentrated in the range [0,1100], which we\u0026rsquo;ll use later to analyze AOV.\nAnalysis 2: Time Series of Order Amount Analysis From a quick scroll of the original data, we can see that shop_id 42 and 78 are outliers that have large order values that potentially skew the AOV (cause the AOV to be much larger than it supposed to be). This behavior can also be observed from the graph of Daily Order Amount by Shop.\nCode fig = px.line(data,y=\u0026#34;order_amount\u0026#34;,x=\u0026#34;created_at\u0026#34;,color=\u0026#34;shop_id\u0026#34;,title=\u0026#34;Daily Order Amount by Shop\u0026#34;) fig.show() fig.write_json(\u0026#34;plot_data/daily_order_amount_by_shop.json\u0026#34;) Graphing the data without outlier shops, we have a more concentrated view of the data.\nCode fig = px.line(data[~data.shop_id.isin([42,78])],y=\u0026#34;order_amount\u0026#34;,x=\u0026#34;created_at\u0026#34;,color=\u0026#34;shop_id\u0026#34;,title=\u0026#34;Daily Order Amount by non-outlier Shop\u0026#34;) fig.show() fig.write_json(\u0026#34;plot_data/daily_order_amount_by_shop_no_outlier.json\u0026#34;) In fact, we can see that other 98 shops don\u0026rsquo;t have any order that exceed $1100, which agrees with previous analysis on order_amount empirical distribution.\nPart B With previous analyses, I propose 3 solutions to analyze AOV:\nMedian of order amount as AOV As the first analysis show that the order amount is left skewed and the count of large order_amount is small, we can use the median as the AOV. This requires minimum change in code base and is the simplest solution. However, to implement this we need to communicate with stakeholders to brief the reason of using median. This metric doesn\u0026rsquo;t consider temporary spikes (example: shops suddenly got an influx of orders for Christmas, etc.) AOV using arithmetic mean with order_amount capped at $1100. Depends on the business needs (i.e. AOV per product category for small businesses), we can remove large outlier shops. Doesn\u0026rsquo;t change the metric type Alienate large business users, and doesn\u0026rsquo;t consider temporary spikes In larger scale, we need to build a robust solution to detect and remove outliers from reported AOV. AOV using arithmetic mean with shop_id 42 and 78 removed as outliers. Same pros and cons as the 2nd solution, except that we will able to catch temporary spikes in order amount. We can also add other interesting additions to the above solutions (won\u0026rsquo;t be implemented for this challenge):\nPer-shop AOV Report to shop owner for their personalized metrics Shop performance metrics on platform Daily AOV with outliers removed Analyzing trends of order amount by day Identify peak days and potential bottoms Part C Median of order amount as AOV (Note that this median is on the whole dataset with outliers included)\nCode print(\u0026#34;Median as AOV: ${}\u0026#34;.format(data.order_amount.median())) Median as AOV: $284.0 Arithmetic mean of order amount with outliers removed\nCode print(\u0026#34;Arithmetic mean as AOV on capped order amount of $1100: ${}\u0026#34;.format(data.order_amount[data.order_amount\u0026lt;=1100].mean())) print(\u0026#34;Arithmetic mean as AOV with outlier shops removed: ${}\u0026#34;.format(data[~data.shop_id.isin([42,78])].order_amount.mean())) Arithmetic mean as AOV on capped order amount of $1100: $301.83704904742604 Arithmetic mean as AOV with outlier shops removed: $300.1558229655313 Note: For median, even with outliers removed, we still get the same result as above\nCode print(\u0026#34;Median as AOV on capped order amount of $1100: ${}\u0026#34;.format(data.order_amount[data.order_amount\u0026lt;=1100].median())) print(\u0026#34;Median as AOV with outlier shops removed: ${}\u0026#34;.format(data[~data.shop_id.isin([42,78])].order_amount.median())) Median as AOV on capped order amount of $1100: $284.0 Median as AOV with outlier shops removed: $284.0 Conclusion: Depends on business need, either \u0026ldquo;Median as AOV\u0026rdquo; or \u0026ldquo;Arithmetic mean as AOV with outlier shops\u0026rdquo; can be used to analyze AOV.\nQuestion 2 Queries for this question are fully compatible with the website providing the dataset (w3schools.com).\nHow many orders were shipped by Speedy Express in total? Result: 54 orders were shipped by Speedy Express in total SELECT count(*) FROM Orders LEFT JOIN Shippers ON Orders.ShipperID = Shippers.ShipperID WHERE Shippers.ShipperName=\u0026#34;Speedy Express\u0026#34;; What is the last name of the employee with the most orders? Result: Last name of the employee with the most orders (40) is Peacock SELECT TOP 1 count(*) as OrderCount, Employees.LastName FROM Orders LEFT JOIN Employees ON Orders.EmployeeID=Employees.EmployeeID GROUP BY Employees.EmployeeID,Employees.LastName ORDER BY count(*) DESC; What product was ordered the most by customers in Germany? Result: Product was order the most (5 times) by customers in Germany is Gorgonzola Telino SELECT TOP 1 count(*) as OrderCount, Products.ProductName FROM (((Orders INNER JOIN OrderDetails ON Orders.OrderID=OrderDetails.OrderID) INNER JOIN Products ON OrderDetails.ProductID=Products.ProductID) INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID) WHERE Country = \u0026#34;Germany\u0026#34; GROUP BY Products.ProductID,Products.ProductName ORDER BY count(*) DESC; ","permalink":"https://n0k0m3.github.io/projects/shopify/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/n0k0m3/n0k0m3.github.io/blob/main/_projects/shopify/datascience.ipynb\"\u003eDownload Notebook\u003c/a\u003e{: .btn .btn\u0026ndash;info }\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e All graphs and plots are interactive. Feel free to zoom, pan, and edit the graphs for more granular details.\u003c/p\u003e\n\u003ch2 id=\"question-1\"\u003eQuestion 1\u003c/h2\u003e\n\u003ch3 id=\"part-a\"\u003ePart A\u003c/h3\u003e\n\u003cdetails\u003e\n\u003csummary\u003eCode\u003c/summary\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003epandas\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003epd\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eplotly.express\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003epx\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epx\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edefaults\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewidth\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e600\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epx\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edefaults\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheight\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e400\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/details\u003e\n\u003cp\u003eA quick view (first 5 rows) of the data\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003eCode\u003c/summary\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eread_csv\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;https://docs.google.com/spreadsheets/d/16i38oonuX1y1g7C_UAmiK9GkY7cS-64DfiDMNiR41LM/edit#gid=0\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereplace\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;/edit#gid=\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;/export?format=csv\u0026amp;gid=\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/details\u003e\n\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\u003cpre\u003e\u003ccode\u003e.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e","title":"Shopify Fall 2022 Data Science Intern Challenge"}]